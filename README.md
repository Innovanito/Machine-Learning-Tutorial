# Machine-Learning-Tutorial with Code Basics

I learned Mechine-Learning with youtuber Code Basics

And Here is the things that I learned through it

Bagging: A technique for ensemble learning, where multiple models are trained on different subsets of the data to reduce variance and improve performance.

Decision Tree: A tree-like model that makes decisions by recursively splitting the data based on features, with each split maximizing the information gain.

Hyperparameter Tuning: The process of selecting the best hyperparameters for a model by trying different combinations and evaluating performance.

K-Fold Cross Validation: A technique for evaluating a model's performance by splitting the data into K subsets, training on K-1 subsets and evaluating on the remaining subset, and repeating K times with a different subset as the test set each time.

K-Means Clustering Algorithm: A clustering algorithm that partitions data into K clusters based on their similarity, where K is a pre-defined number.

K-Nearest Neighbors Classification: A classification algorithm that predicts the class of a data point based on the class of its K nearest neighbors in the training data.

L1 and L2 Regularization: Techniques for preventing overfitting in a model by adding a penalty term to the loss function that encourages small weights (L1) or small weights squared (L2).

Logistic Regression: A classification algorithm that models the probability of a binary outcome (e.g., yes/no) as a function of the input features.

Naive Bayes Classifier Algorithm: A classification algorithm based on Bayes' theorem, which models the probability of a class given the input features.

Principle Component Analysis (PCA): A technique for reducing the dimensionality of data by finding the principal components that explain the most variance.

Random Forest: An ensemble learning technique that trains multiple decision trees on random subsets of the data and features, and combines their predictions.

Support Vector Machine: A classification algorithm that finds the hyperplane that maximally separates the data into different classes, or that maximizes the margin between the classes.


## Thoughs about learning the concept of Mechine-Learning

In learning process of Mechine-Learning, I first learn the core concept of the theory and I do exercise how does this theory apply to the model.

I was very surprised that I can use various kind of theory to analyze the data, and make model using many kinds of theory.

For example, when I want to predict what kind of flower is the given picture is, I can use Support Vector Mechine, and Random Forest, and K-Means Clustering.

But I found it interesting because even though I can make models using various kinds of theory, the Prediction Accuracy varies from data to data.

In order to increase the prediction accuracy, I learned that I have to tune parameter and hyperparameter of the each algorithm.
